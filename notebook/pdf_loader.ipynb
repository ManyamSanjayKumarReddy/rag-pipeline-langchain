{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83cc740a",
   "metadata": {},
   "source": [
    "\n",
    "RAG Pipeline- Date Injestion to Vector DB Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1bbffa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "81d3e976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 PDF files to process\n",
      "\n",
      "processing: Manual for Skill registry - Industry Registration and Login.pdf\n",
      "Loaded 17 pages\n",
      "\n",
      "processing: ibm vmware broker documnet (2).pdf\n",
      "Loaded 18 pages\n",
      "\n",
      "processing: React Check List.pdf\n",
      "Loaded 4 pages\n",
      "Total Documents Loaded: 39\n"
     ]
    }
   ],
   "source": [
    "## read all the pdfs inside the directory\n",
    "\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all the pdf files from the directory\"\"\"\n",
    "\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "\n",
    "    # find all pdf files recursivley\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "\n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nprocessing: {pdf_file.name}\")\n",
    "\n",
    "        try:\n",
    "            loader = PyMuPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "\n",
    "            # add source info to meta data\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "            \n",
    "            all_documents.extend(documents)\n",
    "\n",
    "            print(f\"Loaded {len(documents)} pages\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error {e}\")\n",
    "    \n",
    "    print(f\"Total Documents Loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "# process all the pdf documents from the directory\n",
    "all_pdf_documents = process_all_pdfs(\"../data/pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3b602de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(all_pdf_documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e0878216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Splitting into Chunks\n",
    "\n",
    "def split_documents(documents, chunk_size=1000,chunk_overlap=200):\n",
    "    \"\"\"Split the documents into smaller chunks for better rag performance\"\"\"\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunk_size,\n",
    "        chunk_overlap = chunk_overlap,\n",
    "        length_function = len,\n",
    "        separators=[\"\\n\\n\",\"\\n\",\" \", \"\"]\n",
    "    )\n",
    "\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "\n",
    "    # sample chunk\n",
    "    if split_docs:\n",
    "        print(\"\\nExample Chunk: \")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"metadata: {split_docs[0].metadata}\")\n",
    "\n",
    "    return split_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f8ca5224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 39 documents into 41 chunks\n",
      "\n",
      "Example Chunk: \n",
      "Content: Key Features of TNSKILL Registry \n",
      "1. Simple Registration & Login \n",
      " \n",
      "Register using Company / Industry details with admin approval \n",
      "process. \n",
      " \n",
      "Multiple login methods: Email/Password, Mobile/Password...\n",
      "metadata: {'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-23T09:29:52+00:00', 'source': '../data/pdf/Manual for Skill registry - Industry Registration and Login.pdf', 'file_path': '../data/pdf/Manual for Skill registry - Industry Registration and Login.pdf', 'total_pages': 17, 'format': 'PDF 1.5', 'title': '', 'author': 'AVP-Portal Naanmudhalvan', 'subject': '', 'keywords': '', 'moddate': '2025-09-23T09:29:52+00:00', 'trapped': '', 'modDate': 'D:20250923092952Z', 'creationDate': \"D:20250923092952+00'00'\", 'page': 0, 'source_file': 'Manual for Skill registry - Industry Registration and Login.pdf', 'file_type': 'pdf'}\n"
     ]
    }
   ],
   "source": [
    "chunks = split_documents(all_pdf_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a421a7d",
   "metadata": {},
   "source": [
    "### embeddings and vectorStore DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2037179f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List,Dict,Any,Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f8e70713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOading embedding model: all-MiniLM-L6-v2\n",
      "Model Loaded Successfully. Embedded Dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x7104956af020>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding generation using sentence transformer\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "\n",
    "        Initilze the embedding manager\n",
    "\n",
    "        Args:\n",
    "            model_name : Huggingface model name for sentence embeddings\n",
    "        \n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the sentence transformer model\"\"\"\n",
    "\n",
    "        try:\n",
    "            print(f\"LOading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "\n",
    "            print(f\"Model Loaded Successfully. Embedded Dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"error loading model {self.model_name}:  {e}\")\n",
    "            raise\n",
    "    \n",
    "    def generate_embeddings(self,texts:List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate Embeddings for a list of texts\n",
    "\n",
    "        Args:\n",
    "            texts: List of text strings to embbed\n",
    "        \n",
    "        Returns:\n",
    "            numpy array of embeddings with shape (len(texts), embedding_dim)\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not LOaded\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts\")\n",
    "        embeddings = self.model.encode(texts,show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "\n",
    "        return embeddings\n",
    "    \n",
    "    ## initilize the embedding maanger\n",
    "\n",
    "embedding_manager = EmbeddingManager()\n",
    "embedding_manager\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bc1cf306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector store initilized. Collection : pdf_documents\n",
      "Exisiting documents in collection 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x7104954b7230>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Vector Store\n",
    "\n",
    "class VectorStore:\n",
    "    \"\"\"Manages document embeddings in chromadb vector store\"\"\"\n",
    "\n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "        \"\"\"\n",
    "\n",
    "        Initilize the vector store\n",
    "\n",
    "        Args:\n",
    "            collection_name : Name of the chromadb collection\n",
    "            persist_directory: Directory to persist the vector store\n",
    "        \"\"\"\n",
    "\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initilize_store()\n",
    "\n",
    "    def _initilize_store(self):\n",
    "        \"\"\"Initilize chromadb client and collection\"\"\"\n",
    "        try:\n",
    "            # create persistant chromadb client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "\n",
    "            # get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name = self.collection_name,\n",
    "                metadata = {\"description\": \"PDF Document embeddings for RAG\"}\n",
    "            )\n",
    "\n",
    "            print(f\"vector store initilized. Collection : {self.collection_name}\")\n",
    "            print(f\"Exisiting documents in collection {self.collection.count()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ERror initilizing vector store {e}\")\n",
    "            raise\n",
    "    \n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add Documents and their embeddings to the vector store \n",
    "\n",
    "        Args:\n",
    "            document:List of Langchain documents\n",
    "            embeddings: corrosponding embeddings of the document\n",
    "        \"\"\"\n",
    "\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"number of documents should match the embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector store\")\n",
    "\n",
    "        # prepare data for chromaDB\n",
    "\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list  = []\n",
    "\n",
    "        for i , (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "\n",
    "            # create unique id\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "\n",
    "            # prepare meta data\n",
    "\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            # Document Content\n",
    "            documents_text.append(doc.page_content)\n",
    "\n",
    "            # embeddings\n",
    "\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "\n",
    "        # add to collections\n",
    "\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings= embeddings_list,\n",
    "                metadatas = metadatas,\n",
    "                documents= documents_text\n",
    "            )\n",
    "\n",
    "            print(f\"successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")    \n",
    "        except Exception as e:\n",
    "            print(f\"ERror adding documents to vector store\") \n",
    "            raise   \n",
    "        \n",
    "vectorstore = VectorStore()\n",
    "vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f13e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 41 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:01<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (41, 384)\n",
      "Adding 41 documents to vector store\n",
      "successfully added 41 documents to vector store\n",
      "Total documents in collection: 41\n"
     ]
    }
   ],
   "source": [
    "# convert text to embeddings\n",
    "texts = [doc.page_content for doc in chunks]\n",
    "\n",
    "# generate the embeddings\n",
    "embeddings = embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "# store in the vector store\n",
    "vectorstore.add_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "28faed0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.11711098, -0.008301  , -0.0131607 , ...,  0.05842284,\n",
       "        -0.00030941, -0.01528272],\n",
       "       [-0.03013904,  0.04563437,  0.03822807, ...,  0.03544493,\n",
       "        -0.02014904,  0.08997379],\n",
       "       [-0.06649102,  0.00303016, -0.07202322, ...,  0.06313304,\n",
       "         0.02408824, -0.03392732],\n",
       "       ...,\n",
       "       [-0.07805411,  0.02167594,  0.0020808 , ..., -0.00869135,\n",
       "         0.02055571, -0.015007  ],\n",
       "       [-0.06792241,  0.03647555,  0.0432132 , ..., -0.01364601,\n",
       "         0.0210285 , -0.01765706],\n",
       "       [-0.07062619,  0.02819395, -0.01981876, ..., -0.00553241,\n",
       "        -0.03520308, -0.05317404]], shape=(41, 384), dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#full content of the doc page content\n",
    "# texts \n",
    "\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b95afdc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ec90fff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual Search \n",
      "Click the Manual Search button, select filters such as College Type, Certification, \n",
      "Gender, Placement Status, District/Location, Year of Passing, Branch, College \n",
      "Name, etc., according to your requirements, and then click the Search Now button \n",
      "(highlighted in the image below). \n",
      " \n",
      "Once you click on Search Now, a list of eligible skilled professionals matching your \n",
      "requirements will be displayed.\n",
      "Manual Search \n",
      "Click the Manual Search button, select filters such as College Type, Certification, \n",
      "Gender, Placement Status, District/Location, Year of Passing, Branch, College \n",
      "Name, etc., according to your requirements, and then click the Search Now button \n",
      "(highlighted in the image below). \n",
      " \n",
      "Once you click on Search Now, a list of eligible skilled professionals matching your \n",
      "requirements will be displayed.\n",
      "[ 5.65390810e-02 -5.79172634e-02  1.48853613e-02  2.37953737e-02\n",
      " -4.28575985e-02  2.52832193e-02 -8.22808519e-02 -3.93530577e-02\n",
      " -1.29107952e-01 -2.98143532e-02 -4.54265326e-02 -8.91675428e-02\n",
      " -3.14340778e-02  3.38764465e-03 -9.76881236e-02 -6.61066845e-02\n",
      " -2.00182814e-02  8.38783570e-03  7.26624355e-02 -1.11986607e-01\n",
      "  5.77970631e-02  1.93176698e-02 -1.95925348e-02 -1.16671890e-01\n",
      " -1.82773769e-02 -6.03495538e-02  3.35027203e-02 -7.30915368e-02\n",
      " -2.67461054e-02  2.30273306e-02 -3.03834733e-02  2.32946649e-02\n",
      "  3.55621167e-02  9.01818424e-02  5.68343177e-02 -8.25273544e-02\n",
      " -2.50568446e-02  8.78693163e-02  9.05263200e-02 -3.32525074e-02\n",
      " -6.52251616e-02 -3.85514349e-02  4.39664023e-03  1.22095109e-03\n",
      "  5.14595024e-02  2.19254363e-02 -5.68131655e-02 -8.22208524e-02\n",
      "  5.58091961e-02  2.85962783e-02 -9.31895524e-02 -5.44460900e-02\n",
      "  1.28816655e-02 -2.09320858e-02 -2.96621677e-02  4.83868718e-02\n",
      "  4.63073514e-03 -2.58013289e-02 -8.30410719e-02  1.44498339e-02\n",
      "  7.35871261e-03 -6.09465502e-03 -9.41943377e-03 -1.67456288e-02\n",
      " -1.81499999e-02  4.01588194e-02  3.45910457e-03  5.30392351e-03\n",
      "  1.10982344e-01 -7.69514441e-02  3.00320033e-02 -1.41758146e-02\n",
      " -1.31075069e-01 -1.67623081e-03  1.06583767e-01 -2.50520967e-02\n",
      " -1.04616547e-03  3.48691456e-02  3.87455598e-02 -1.21083604e-02\n",
      " -5.67948185e-02  4.68253996e-03 -6.65169284e-02  2.11503077e-02\n",
      "  1.99099649e-02  1.90179963e-02 -3.79442833e-02 -2.51804274e-02\n",
      "  5.63338324e-02 -1.99285848e-03  2.77052913e-02 -4.82266806e-02\n",
      " -7.82639757e-02 -2.63709645e-03 -2.51817307e-03 -2.44831312e-02\n",
      "  2.83942930e-02  1.30268112e-02  6.91450611e-02 -6.84350729e-03\n",
      " -1.01640493e-01 -9.52008963e-02  4.71879356e-03 -1.26079069e-02\n",
      " -1.12014785e-01  7.38757774e-02  4.10544723e-02  2.15831771e-02\n",
      "  3.44860554e-02  1.94414338e-06 -1.35092239e-03  4.35421951e-02\n",
      " -1.03247963e-01 -3.95653509e-02 -1.37082012e-02  5.58592677e-02\n",
      "  4.23317775e-04  7.57582905e-03  4.01512049e-02  5.50159626e-02\n",
      " -8.95030349e-02  2.27563381e-02 -9.05700400e-02 -1.00022577e-01\n",
      "  1.81888919e-02  4.65199677e-03 -8.05950761e-02 -1.91337824e-33\n",
      "  4.80157882e-02  4.39108759e-02  1.44175617e-02  6.87311590e-02\n",
      " -5.66825941e-02 -5.07870205e-02  9.14527178e-02  7.10195900e-06\n",
      "  5.46142310e-02 -2.18940265e-02 -1.82702057e-02  7.41341934e-02\n",
      " -5.64157143e-02  2.90094893e-02 -1.36675565e-02  1.63115766e-02\n",
      " -1.84065606e-02  2.08433773e-02 -6.10870533e-02  8.21515173e-02\n",
      "  3.81655358e-02 -1.31025426e-02 -3.24590802e-02 -2.41605602e-02\n",
      " -6.61378121e-03  4.45756279e-02 -1.06108962e-02  3.57425064e-02\n",
      "  6.14290871e-02  3.56382504e-02  2.20130924e-02 -7.27915689e-02\n",
      "  4.86066914e-04 -7.54126832e-02  1.01696111e-01  1.04507782e-01\n",
      " -7.44174793e-03  6.71906620e-02  7.96129555e-02 -2.89790668e-02\n",
      " -2.06019823e-02 -3.32700147e-04  5.69377467e-02  3.14465128e-02\n",
      " -9.65290295e-04 -1.45118823e-02  6.33806875e-03 -7.75520410e-03\n",
      "  8.26692209e-02  4.80626710e-02 -2.88043097e-02 -5.92227885e-03\n",
      " -8.25496688e-02 -1.71512458e-02  7.65616894e-02  1.45758558e-02\n",
      "  5.77866957e-02  1.04417935e-01 -4.48164009e-02  1.10467039e-02\n",
      "  2.55966850e-04 -2.37101726e-02 -2.47886200e-02  3.53857130e-02\n",
      " -1.31816650e-03 -4.80561592e-02  9.46968794e-03 -3.03646605e-02\n",
      "  1.52656943e-01 -4.66136187e-02 -7.42374435e-02 -2.32000817e-02\n",
      "  3.25639583e-02  3.27390730e-02 -2.88160099e-03  5.66537306e-02\n",
      " -2.61259973e-02 -9.25740111e-04  2.73316689e-02  1.25428028e-02\n",
      "  1.06550483e-02  1.32587738e-04 -4.06557843e-02  3.36049795e-02\n",
      "  1.30750254e-01 -4.21866216e-02  4.71851490e-02 -3.10214385e-02\n",
      "  6.61036298e-02  1.70735305e-03  3.23147550e-02  2.91656535e-02\n",
      " -8.11098590e-02  7.93362334e-02  1.62942316e-02 -8.39747083e-34\n",
      "  3.41921858e-02 -3.29372287e-02  3.52057293e-02  1.46196177e-02\n",
      "  4.50958945e-02 -6.68537850e-03  6.73191324e-02 -5.22970967e-02\n",
      "  9.28305089e-02  1.21869217e-03  5.55993207e-02 -2.54669357e-02\n",
      " -2.62104217e-02  1.17411083e-02 -1.03955761e-01  4.69270498e-02\n",
      " -1.52445629e-01  9.83960703e-02  5.89565746e-03  5.25774658e-02\n",
      " -7.59419426e-03  1.64528880e-02 -2.25120354e-02  4.13482601e-04\n",
      " -2.36466099e-02 -7.02650612e-03  3.17649357e-03 -6.63137980e-05\n",
      "  5.15419384e-03  1.99781209e-02  7.82992225e-04 -3.91789488e-02\n",
      " -6.18201084e-02  5.89103475e-02 -4.85478751e-02 -6.44493625e-02\n",
      "  7.13777635e-03  7.67075866e-02 -6.64475374e-03  1.04227595e-01\n",
      " -1.97353549e-02 -3.44579928e-02 -3.90977375e-02 -2.95088440e-02\n",
      " -3.00538111e-02 -5.05186915e-02  1.81455165e-02 -1.85819939e-02\n",
      "  3.38793620e-02 -1.73650980e-02  3.36938277e-02 -2.89586298e-02\n",
      " -6.32810080e-03 -1.68996025e-02  8.03489387e-02  3.45380381e-02\n",
      "  4.40458246e-02 -3.71050951e-03  5.66680171e-03  7.81436414e-02\n",
      "  2.81447824e-03  5.11661544e-03 -2.20275912e-02  4.41489890e-02\n",
      " -8.13550130e-03 -8.13616961e-02 -2.98481360e-02  1.05046416e-02\n",
      " -1.04338773e-01  5.40534779e-02 -7.87154585e-03 -6.35517910e-02\n",
      "  5.68326823e-02 -6.43422827e-02 -6.65197000e-02 -3.63394208e-02\n",
      "  3.39670829e-03  5.86450100e-02  1.15625141e-02  2.12154090e-02\n",
      " -1.83354001e-02  2.30770949e-02 -3.93723100e-02  8.46121013e-02\n",
      "  5.42139588e-03  5.42656966e-02 -9.74794291e-03 -1.36451870e-02\n",
      "  1.41493278e-02 -7.28322938e-02 -5.49049191e-02 -4.88354564e-02\n",
      " -2.97468901e-02 -8.64109993e-02 -8.07379037e-02 -3.82427707e-08\n",
      " -4.57031373e-03  3.53055410e-02 -8.33488256e-02  3.67570817e-02\n",
      "  2.51859576e-02  6.35275915e-02 -4.34176661e-02  3.65024507e-02\n",
      " -3.29093896e-02  4.37829383e-02 -9.18243267e-03 -4.30216640e-02\n",
      "  1.91917736e-02 -4.29729670e-02  8.72043371e-02  2.14952398e-02\n",
      " -3.75365876e-02  8.98831487e-02 -2.39647646e-03  2.83208652e-03\n",
      "  5.21604419e-02 -5.46742119e-02 -1.80367306e-02  9.73054916e-02\n",
      " -2.07665861e-02  5.97763881e-02 -7.15029463e-02  2.32506241e-03\n",
      " -7.46373460e-02  6.80811927e-02  3.63119133e-02  4.04987037e-02\n",
      "  4.47082110e-02 -1.06579244e-01  6.98397458e-02  3.79675403e-02\n",
      " -8.77279043e-03 -2.45006084e-02 -1.16419140e-02 -1.95441674e-02\n",
      "  7.11828063e-04 -4.64967713e-02 -3.07309628e-02 -2.42588180e-03\n",
      "  9.09754261e-02  2.88656540e-02  3.94970663e-02  4.50993748e-03\n",
      "  2.11220477e-02  8.79473910e-02 -9.82869416e-03 -8.11536238e-02\n",
      " -2.06648726e-02 -8.37982669e-02 -9.54596028e-02  8.81798044e-02\n",
      "  1.99386310e-02 -3.31323706e-02 -5.84709598e-03 -7.98468515e-02\n",
      "  6.65671900e-02  3.34205814e-02  6.24954887e-03  3.74298245e-02]\n"
     ]
    }
   ],
   "source": [
    "print(chunks[10].page_content)\n",
    "print(texts[10])\n",
    "print(embeddings[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3d3ccc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
